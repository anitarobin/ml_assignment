---
title: "Prediction Assignment"
author: "Anita Robin"
date: "13 March 2016"
output: html_document
---



## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (refer Weight Lifting Exercise Dataset).

Participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in ﬁve diﬀerent fashions: exactly according to the speciﬁcation **(Class A)**, throwing the elbows to the front**(Class B)**, lifting the dumbbell only halfway **(Class C)**, lowering the dumbbell only halfway **(Class D)** and throwing the hips to the front **(Class E)**. 

Our aim is to apply suitable machine learning techniques to predict the manner in which they did the exercise. 

---

## Getting the Data


```{r , echo = FALSE}
#loading the required library
        library(caret)
        library(ggplot2)
        library(randomForest)
#loading the training data
        training = read.csv("pml-training.csv")
#loading the test data set
        testing = read.csv("pml-test.csv")
```

---

## The Data

```{r }

#dimensions of training data
        dim(training)
#dimensions of test data
        dim(testing)
#closer look at feature set
        colnames(training)[1:10]
        colnames(training)[50:60]
        colnames(training)[100:110]
        colnames(training)[150:160]
        
#The "classe" variable in the training set which is of interest
        summary(training$classe)
```
---

## Exploratory Data Analysis

###Looking at the sheer volume of data, it is clear that we need to narrow down the potential predictors for our study. A closer look at the features, using the pattern matching capabilities of R, helps us shortlist variables which we can then use for exploratory data analysis using relevant plots.
 
###To illustrate, we would assume that measurement features pertaining to forearm/arm will have a higher influence on classification between Class A (the correct way)and Class B(common mistake of putting elbows to the front). Lets do a quick data analysis to check this.

```{r, echo=FALSE}
#pattern matching for columns containing arm/forearm
        l<-grep("arm",colnames(training))
#getting the relevant column names
        colnames(training[grep("arm",colnames(training))])
#getting rid of nas        
        training[is.na(training)] <- 0
        
#plot1 classe against pitch_forearm
        p <- ggplot(data=training, aes(classe,pitch_forearm))
        p + geom_boxplot(aes(fill =classe))
#plot2 classe against gyros_arm_x
        p <- ggplot(data=training, aes(classe,gyros_arm_x))
        p + geom_boxplot(aes(fill =classe))
        
```

---

## Machine Learning Technique

###Looking at the above, it seems that **an approach involving a step by step binary classification between class A vs. each of the other classes ** may yield us a list of variables that we can use to finally train our final model for classifying the data.

###To check the feasibilty of the above approach, we generated a subset of the data to include only the variables of interest(using exploratory data plots, not included in this document, in the interest of brevity). Then we further split up data into training and test(for cross validation). A look at the accuracy on the taining and cross validation set will help us decide in favour of/against this approach. We plan to use random forest to train our classifier since its the most efficient one for non linear settings. 

---

```{r, echo=FALSE}
#subset the data - include only class A and class B and feature set for 
#arm/forearm/dumbbell excluding all statistical variables
        set.seed(1234)
        trainings<-training[training$classe %in% c("A","B"),
                    c("accel_dumbbell_x","accel_dumbbell_y",
                       "magnet_dumbbell_x","gyros_arm_x","gyros_arm_y",
                       "pitch_arm","pitch_forearm","magnet_forearm_x",
                       "magnet_forearm_y","accel_forearm_x","classe")]
#ignoring all other classes
        trainings$classe=factor(trainings$classe)
#creating training and cross validation sets
        inTrain = createDataPartition(trainings$classe, p = 3/4)[[1]]
        train = trainings[ inTrain,]
        test = trainings[ -inTrain,]
#using random forest to train our classifier
        modFit1 <- train(classe ~., 
              method = "rf",
              data=train,
              verbose = FALSE,
              trControl=trainControl(method="cv"),number=3
              )
        
#final model parameters
        modFit1     
#getting prediction 
        pred1<-predict(modFit1,test)
#getting accuracy
        test$rightPred <- (pred1 == test$classe) 
        accuracy <- sum(test$rightPred)/nrow(test)
        accuracy
#looking for strong correlations to eliminate if neccessary 
#commented here for the sake of brevity
 #           cor(trainings[sapply(trainings, function(x) !is.factor(x))])

```

###The accuracy figure of ~98% is very encouraging. Hence we go ahead and fit models for each of the other classes. We end up with a variable list that we use to run our final model on, again using random forest.


```{r, eval=FALSE}
###class C
#pattern matching for columns containing dumbbell
        l<-grep("dumbbell",colnames(training))

#getting the relevant column names
        colnames(training[grep("dumbbell",colnames(training))])

#exploratory plots
        p <- ggplot(data=training, aes(classe,yaw_dumbbell))
        p + geom_boxplot(aes(fill =classe))

#subsetting the data
        trainings<-training[training$classe %in% c("A","C"),
                    c("accel_dumbbell_x","accel_dumbbell_y",
                      "magnet_dumbbell_x","roll_dumbbell",
                      "yaw_dumbbell","classe")]
        trainings$classe=factor(trainings$classe)
        inTrain = createDataPartition(trainings$classe, p = 3/4)[[1]]
        train = trainings[ inTrain,]
        test = trainings[ -inTrain,]
#building the classifier
        modFit2 <- train(classe ~., 
                method = "rf",
                data=train,
                verbose = FALSE,
                trControl=trainControl(method="cv"),number=3
)
#predictions and accuracy
        pred1<-predict(modFit2,test)
        test$rightPred <- (pred1 == test$classe) 
        accuracy <- sum(test$rightPred)/nrow(test)
        accuracy 

#Repeat for class D
        
#subsetting for training and validation set
        trainings<-training[training$classe %in% c("A","D"),
                    c("accel_dumbbell_x","accel_dumbbell_y",
                      "magnet_dumbbell_x","roll_dumbbell",
                      "yaw_dumbbell","classe")]
        trainings$classe=factor(trainings$classe)
        inTrain = createDataPartition(trainings$classe, p = 3/4)[[1]]
        train = trainings[ inTrain,]
        test = trainings[ -inTrain,]

#training the classifier
        modFit3 <- train(classe ~., 
                 method = "rf",
                 data=train,
                 verbose = FALSE,
                 trControl=trainControl(method="cv"),number=3
)
#testing the accuracy
        pred1<-predict(modFit3,test)
        test$rightPred <- (pred1 == test$classe) 
        accuracy <- sum(test$rightPred)/nrow(test)
        accuracy 

#Repeat for class E
        
 #pattern matching for columns containing belt
        l<- grep("belt",colnames(training))
        
#exploratory plots
        p <- ggplot(data=training, aes(classe,magnet_belt_z))
        p + geom_boxplot(aes(fill =classe))

#subsetting the data
        trainings<-training[training$classe %in% c("A","E"),
                    c("accel_belt_x","accel_belt_y","accel_belt_z",
                      "roll_belt","pitch_belt","yaw_belt",
                      "magnet_belt_z","magnet_belt_x",
                      "magnet_belt_y","classe")]
        trainings$classe=factor(trainings$classe)
        inTrain = createDataPartition(trainings$classe, p = 3/4)[[1]]
        train = trainings[ inTrain,]
        test = trainings[ -inTrain,]
#checking for correlations
        cor(trainings[sapply(trainings, function(x) !is.factor(x))])
#building the model
        modFit4 <- train(classe ~., 
                 method = "rf",
                 data=train,
                 verbose = FALSE,
                 trControl=trainControl(method="cv"),number=3
        )
#testing the accuracy
        pred1<-predict(modFit4,test)
        test$rightPred <- (pred1 == test$classe) 
        accuracy <- sum(test$rightPred)/nrow(test)
        accuracy
```

## Final model selection

### We have a comparable accuracy for class B and E (around 98%) and slightly lower for class C and D which is understandable both relate to the lifting/lowering of dumbbell only halfway and hence the classifier will be influenced by the same predictors. 
### We will now combine all above models by including all the predictors used and train the model ,once again, using random forest with cross validation, on our original training dataset which has been split into training and test(for validation).

```{r }

#original training set - omit nas
        set.seed(1234)
        training[is.na(training)] <- 0
        inTrain = createDataPartition(training$classe, p = 3/4)[[1]]
        train = training[ inTrain,]
        test = training[ -inTrain,]

#building the model with combined predictors
        finalModel <- train(classe ~ accel_belt_x+accel_belt_y+accel_belt_z+
                 roll_belt+pitch_belt+yaw_belt+
                 magnet_belt_z+magnet_belt_x+
                 magnet_belt_y+accel_dumbbell_x+accel_dumbbell_y+
                 magnet_dumbbell_x+gyros_arm_x+gyros_arm_y+
                 pitch_arm+pitch_forearm+magnet_forearm_x+
                 magnet_forearm_y+accel_forearm_x+
                 roll_dumbbell+yaw_dumbbell,
                method = "rf",
                data=train,
                verbose = FALSE,
                trControl=trainControl(method="cv"),number=3
                )
#using the final model to test predictions against our validation set
        pred1<-predict(finalModel,test)
        test$rightPred <- (pred1 == test$classe) 
        accuracy <- sum(test$rightPred)/nrow(test)
        
#Accuracy of the classifier
        accuracy
        
#Final model
        finalModel
```
---

##Conclusion

###Using random forest iteratively using binary classification between class A and each of the other classes in each step leads us to a very good classifier with an accuracy of ~98%. We expect an out of sample error rate of less than 2%.
